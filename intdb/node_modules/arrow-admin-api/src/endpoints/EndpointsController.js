const async = require('async');
const path = require('path');
const requester = require('@axway/requester');
const YAML = require('js-yaml');
const {
	isSafeFilename,
	mkdirIfNotExists,
	mkdirIfNotExistsAsync,
	safeFilename,
	writeFile,
	writeUniqueFile
} = require('../util');
const OpenAPIUtils = require('../openapi-utils');
const sampleFromSchema = require('./mockUtils');
const ModelEndpointGenerator = require('./ModelEndpointGenerator');
const { BadRequestError,
	NotFoundError,
	ForbiddenError,
	PayloadTooLargeError,
	InternalServerError,
	ExtendableError
} = require('../errors');
const afs = require('../afs');
const jsonRegx = /application\/(?:json|.\*\+json)$/;

/**
 * This is the only spec type we were using prior to introducing protocol
 * triggers. With protocol triggers we can import and work with multiple
 * spec types and those types provide their own way to validate and save.
 * For this legacy type we have built in validation and saving and most probably
 * we will get rid of this legacy type in long run.
 */
const LEGACY_SPEC_TYPE = 'endpoint';
const DYNAMIC_SPEC_TYPE = 'dynamic';

const makeParameter = (value, param) => {
	let type;
	if (typeof value === 'string') {
		type = 'string';
	} else if (typeof value === 'number') {
		type = 'number';
	} else if (typeof value === 'boolean') {
		type = 'boolean';
	} else if (value instanceof Array) {
		type = 'array';
	} else if (value === null) {
		type = 'null';
	} else {
		type = 'object';
		value = value || {};
	}
	return {
		...param,
		value: JSON.stringify(value, null, 4),
		type
	};
};

const mockResponse = (swagger, operationId, code, response) => {
	const flow = {
		schemaVersion: '4',
		info: {
			name: operationId
		},
		parameter: {
			properties: {
				params: {
					type: 'object',
					description: 'The parameters from the endpoint.'
				},
				request: {
					type: 'object',
					description: 'The HTTP request.'
				},
				config: {
					type: 'object',
					description: 'The service\'s configuration'
				},
				env: {
					type: 'object',
					description: 'The host OS environment'
				}
			},
			additionalProperties: false,
			required: [
				'params',
				'request',
				'config',
				'env'
			]
		},
		start: 'http.setresponse.1',
		nodes: {
			'http.setresponse.1': {
				type: 'nodehandler://axway-flow/http',
				method: 'setresponse',
				name: 'Set HTTP Response',
				parameters: [],
				outputs: {
					next: {
						context: '$.response'
					}
				}
			}
		}
	};
	const parameters = flow.nodes['http.setresponse.1'].parameters;

	parameters.push({
		name: 'status',
		value: code === 'default' ? '200' : code,
		type: 'number'
	});
	if (response.headers) {
		const headers = Object.keys(response.headers);
		const headerObj = headers.reduce((acc, cur) => {
			const name = cur.toLowerCase();
			if (name === 'location') {
				acc[name] = '/example/location';
			} else if (name === 'content-type') {
				acc[name] = 'application/json; charset=utf8';
			} else {
				acc[name] = sampleFromSchema(response.headers[cur], swagger);
			}
			return acc;
		}, {});
		parameters.push(makeParameter(headerObj, { name: 'headers' }));
	}
	if (response.schema) {
		const sample = sampleFromSchema(response.schema, swagger);
		parameters.push(makeParameter(sample, { name: 'body' }));
	}
	return flow;
};

/**
 * Ensures the swagger document contains the path and method.
 * @param {object} swagger - The Swagger document.
 * @param {string} eppath - The endpoint path.
 * @param {string} method - The endpoint method.
 */
function ensureEndpointMethodExists(swagger, eppath, method) {
	const methodObj = swagger.paths
		&& swagger.paths[eppath]
		&& swagger.paths[eppath][method];
	if (!methodObj) {
		throw new NotFoundError('Endpoint not found');
	}
}

/**
 * Get the `operationId` from a swagger operation.  If the operation does not
 * have an `operationId`, or if the `operationId` is not file-safe, then one
 * will be generated from the path and method options.
 * @param {object} operation - The Swagger operation.
 * @param {string} path - The path to the operation.
 * @param {string} method - The operation verb.
 * @return {string} The operationId.
 */
function getOperationId(operation, path, method) {
	const operationId = operation.operationId;
	if (operationId && isSafeFilename(operationId)) {
		return operationId;
	}
	return OpenAPIUtils.generateFunctionName(path, method);
}

/**
 * The controller implementing the actions for interacting with Endpoints.
 * @public
 */
class EndpointsController {
	/**
     * Create a ProjectController.
     * @param {object} ctx - The context for this controller.
     * @param {express} ctx.app - The express application.
     * @param {string} ctx.prefix - The prefix bound to.
     * @param {APIBuilder} ctx.apibuilder - The API Builder server.
     * @param {Swagger} ctx.swagger - openapi-doc instance.
     * @param {object} controllers - Additional controllers.
     */
	constructor(ctx, controllers) {
		this.ctx = ctx;
		const dir = this.ctx.apibuilder.config.dir || process.cwd();
		this.endpointsDir = path.resolve(path.join(dir, 'endpoints'));
		this.apidocsDir = path.resolve(path.join(dir, 'apidocs'));
		this.openapiDir = path.resolve(path.join(dir, 'apidocs', 'openapi'));
		this.triggersDir = path.resolve(path.join(dir, 'triggers'));
		this.flowsDir = path.resolve(path.join(dir, 'flows'));
		this.logger = this.ctx.apibuilder.logger;
		this.controllers = controllers;
		this.flowManager = this.ctx.apibuilder.flowManager;
		this.importCache = {};
	}

	/**
	 * Cache the loaded spec so we can later get it from here and save it on disk.
	 * @param {*} type - the spec type.
	 * @param {*} spec - the API spec.
	 * @param {Object} options - options stored in the cache
	 * @param {string} options.filename - the filename.
	 * @param {string} options.mimeType - the mime type.
	 * @param {string} options.formatType - file format, either 'json' or 'yaml'.
	 */
	_cacheSpec(type, spec, options) {
		this.importCache[type] = {
			spec, options
		};
	}

	/**
	 * Deletes cache entry by type.
	 *
	 * @param {string} type - the API spec type.
	 */
	_uncacheSpec(type) {
		delete this.importCache[type];
	}

	/**
	 * Return the cached API spec.
	 *
	 * @param {string} type - the spec type.
	 * @returns {Object} the cached API spec.
	 */
	_getCachedSpec(type) {
		return this.importCache[type];
	}

	/**
	 * Explict call to get the ModelGenerator to allow for easier testing.
	 * @param {string} modelName - the model name
	 * @returns {ModelEndpointGenerator}
	 */
	_createModelGenerator(modelName) {
		return new ModelEndpointGenerator(this.ctx.apibuilder, modelName);
	}

	/**
	 * Remove the flow related extensions from the document.
	 * @param {object} swagger - the swagger definition to clean
	 * @returns {object} cleaned swagger doc
	 */
	_cleanSwagger(swagger) {
		return JSON.parse(JSON.stringify(swagger, (key, value) => {
			if ([ 'x-flow', 'x-enabled' ].indexOf(key) === -1) {
				return value;
			}
		}));
	}

	/**
	 * Fetches data from a URL and makes sure that it doesn't exceed the set limit.
	 *
	 * @param {string} url - url to get data from
	 * @param {number} [maxSize=10,000,000] - max download size in bytes
	 * @returns {Promise}
	 */
	async _fetchURLData(url, maxSize = 10 * 1000 * 1000) {
		const { proxy } = this.ctx.apibuilder.config;
		let contentType;
		let size = 0;
		try {
			const response = await requester.request({ method: 'head', url }, {
				proxy,
				limit: {
					download: maxSize
				}
			});
			contentType = response.headers['content-type'];
			size = response.headers['content-length'];
		} catch (ex) {
			// gobble any HEAD errors. we don't care because we were only using
			// HEAD to get the content-length before trying again below.
		}
		if (size && size > maxSize) {
			throw new PayloadTooLargeError(
				`File size too large: Size ${size}B, Max size ${maxSize}B`);
		}

		if (!contentType) {
			// HEAD did not return a content type.  Use a best effort to see if
			// they are requesting something that looks like yaml.
			if (url.endsWith('.yaml') || url.endsWith('.yml')) {
				contentType = 'text/yaml';
			} else if (url.endsWith('.json')) {
				contentType = 'application/json';
			}
		}
		const response = await requester.request({
			url,
			headers: {
				Accept: contentType || 'application/json, text/yaml'
			}
		}, {
			proxy,
			limit: {
				download: maxSize
			},
			decode: false
		});
		if (response.status > 300) {
			throw new Error('Load API did not return successfully.');
		}

		return {
			// note this is not the content-type that was actually returned
			// from the server. also, this is going to default to utf-8 charset,
			// but I don't think it's worth implementing content-type / charset
			// parsing when 99.9% of these documents are going to be utf-8. We
			// can fix it if someone hits it.
			contentType,
			data: response.body.toString()
		};
	}

	/**
	 * Validates spec of legacy type "endpoint". This one is not used for imported
	 * specs via protocol triggers. They provide their own validate functions.
	 *
	 * @param {string} spec the spec to validate
	 * @returns {Object} the parsed spec, and is it valid or not. In
	 * case of parse failure return object with valid property set to false.
	 */
	async _validateEndpoint(spec) {
		let parsedSpec;
		try {
			parsedSpec = YAML.safeLoad(spec);
			await this.ctx.apibuilder.validateSwagger(parsedSpec);
		} catch (err) {
			this.logger.error(err);
			return {
				valid: false
			};
		}
		return {
			valid: true,
			parsedSpec
		};
	}

	/**
	 * Loads api specs from a file or from remote location. Validates the spec
	 * after it is loaded. Previously named 'validateEndpoint'.
	 *
	 * @param {Object} fields - holds the spec location in url property.
	 * @param {Object} files - holds the file metadata if we load from disk.
	 * @param {String} [type] - determines how we validate the spec. Legacy
	 * endpoints validation or api spec validation.
	 * @returns {Object} the parsed spec and a flag to denote is it valid or not.
	 */
	async loadAPISpec(fields, files, type) {
		type = type || LEGACY_SPEC_TYPE;
		const options = {};
		let spec;

		// Step 1. Try to read the file contents and get a mime type
		if (files && files.file) {
			// Step 1.1 Read from disk (uploaded file)
			options.filename = files.file.filename;
			options.mimeType = files.file.mimetype;
			this.logger.trace('reading spec', files.file.file);
			spec = (await afs.readFile(files.file.file)).toString('utf8');
		} else if (fields && fields.url) {
			// Step 1.2 Read from URL
			const cfg = this.ctx.apibuilder.config;
			const limit = cfg && cfg.admin && cfg.admin.request && cfg.admin.request.limit;
			this.logger.trace('fetching spec', fields.url.value);
			const { data, contentType }
				= await this._fetchURLData(fields.url.value, limit);

			spec = data;
			options.filename = path.basename(fields.url.value);
			options.mimeType = contentType;
		} else {
			throw new BadRequestError('Missing parameter "file" or "url"');
		}

		// Step 2. calculate the file format of the spec based on the mimeType
		// and file contents.
		if (jsonRegx.test(options.mimeType) || spec.match(/^\s*{/)) {
			options.formatType = 'json';
		} else {
			options.formatType = 'yaml';
		}

		// Step 3. validate the spec.
		let validatedSpec;
		if (type === LEGACY_SPEC_TYPE) {
			// Legacy endpoints are validated here
			this.logger.trace('validating endpoint', options);
			validatedSpec = await this._validateEndpoint(spec);
		} else {
			// Everything else is handled by the respective plugin for the type
			const apiSpecType = this.ctx.apibuilder._internal.getApiSpecTypes()[type];
			if (!apiSpecType) {
				throw new BadRequestError(`No info for API spec type ${type}`);
			}

			this.logger.trace('validating spec', type, options);
			// REFACTOR: options are not used in plugin validate interface.
			// Consider removing options from here.
			validatedSpec = await apiSpecType.validate(spec, options);
		}

		// Step 4. Cache the spec for saving when subsequent create API is called
		if (validatedSpec.valid) {
			this.logger.trace('caching spec', type, options);
			this._cacheSpec(type, spec, options);
		}

		// Step 5. Return the validated spec (which may have failed to parse)
		return validatedSpec;
	}

	/**
	 * Save loaded and validated specification on disk. Cleans the cache once
	 * this is done.
	 *
	 * @param {Object} params - parameters that comes from the request.
	 * @returns {string} the API type
	 */
	async createAPISpec(params) {
		const type = params.type || LEGACY_SPEC_TYPE;
		const cache = this._getCachedSpec(type);
		if (!cache) {
			throw new BadRequestError(`No imported spec for type "${type}"`);
		}

		// Handle legacy endpoints
		if (type === LEGACY_SPEC_TYPE) {
			const endpoint = await this.createEndpoint({
				swagger: cache.spec,
				formatType: cache.options.formatType,
				mock: params.mock
			});
			this._uncacheSpec(type);
			return endpoint;
		}

		// Everything from this point on is apiSpecs
		const { _internal } = this.ctx.apibuilder;
		const apiSpecType = _internal.getApiSpecTypes()[type];
		if (!apiSpecType) {
			throw new BadRequestError(`No info for API spec type ${type}`);
		}

		const { specId } = params;
		let triggerTypeId = specId;
		// save or update. `cache.options` contain filename, mimeType. mimeType
		// is what is expected by the plugin so be careful to not refactor it to
		// mimetype.
		if (specId) {
			await apiSpecType.update(triggerTypeId, cache.spec, cache.options);
		} else {
			triggerTypeId = await apiSpecType.save(cache.spec, cache.options);
		}

		// clean the cache
		this._uncacheSpec(type);
		return triggerTypeId;
	}

	/**
	 * This method collects info about the spec types - name, the file types
	 * it supports and is there loaded spec from this type.
	 *
	 * @returns {Object} info about API spec types.
	 */
	async getAPISpecTypes() {
		const { _internal } = this.ctx.apibuilder;
		const typesInfo = {};
		// If endpoints are enabled, then the API spec types is legacy
		if (_internal.endpointsEnabled) {
			typesInfo.endpoint = {
				name: 'OpenAPI',
				fileTypes: [ '.json', '.yaml', '.yml' ]
			};
		} else {
			// There are plugged API services
			const apiSpecTypes = _internal.getApiSpecTypes();
			for (const type in apiSpecTypes) {
				const apiSpecType = apiSpecTypes[type];
				typesInfo[type] = {
					name: apiSpecType.name,
					fileTypes: apiSpecType.fileTypes
				};
			}
		}
		return typesInfo;
	}

	/**
	 * Deletes a type of spec with all its associated flows and triggers.
	 *
	 * @param  {string} triggerTypeId - The spec triggerTypeId, e.g. openapi-foo
	 * @async
	 */
	async deleteSpec(triggerTypeId) {
		this.logger.trace(`Deleting specification: ${triggerTypeId}`);

		const apiSpecs = this.ctx.apibuilder._internal.getApiSpecs();
		// Check to see if there's a loaded spec for the type being deleted
		const apiSpec = apiSpecs.find(s => s.triggerTypeId === triggerTypeId);
		if (!apiSpec) {
			throw new NotFoundError(`Spec '${triggerTypeId}' not found.`);
		}
		const triggerManager = this.ctx.apibuilder._internal.getTriggerManager();

		// 1. Get all triggers for this spec
		const { triggers: allTriggers } = triggerManager.getInfo();
		const specTriggers = allTriggers[apiSpec.triggerTypeId];

		// Get a hold of the spec interface
		const allSpecTypes = this.ctx.apibuilder._internal.getApiSpecTypes();
		const specInterface = allSpecTypes[apiSpec.type];

		// 2. Delete all related flows and their triggers
		for (const triggerId in specTriggers) {
			const { flowId } = specTriggers[triggerId];
			// Throws InternalServerError if anything goes wrong
			await this.controllers.flowsController.deleteFlow(flowId);
		}

		try {
			// 3. Delete the spec from disk
			await specInterface.delete(triggerTypeId);
			this.logger.trace(`Deleting specification: ${triggerTypeId} - completed.`);
		} catch (err) {
			this.logger.error(err);
			// security - do not throw with err.message
			throw new InternalServerError();
		}
	}

	createEndpoint(params) {
		const { formatType, swagger, mock } = params;
		return new Promise((resolve, reject) => {
			let parsed;
			try {
				parsed = YAML.safeLoad(swagger);
			} catch (err) {
				// should be no reason why this fails as it was prevalidated.
				this.logger.error(err);
				reject(new InternalServerError());
			}
			const group = safeFilename(parsed.info.title);
			let alltasks = [
				(next) => mkdirIfNotExists(this.endpointsDir, next)
			];
			const bindSwagger = this._cleanSwagger(parsed);
			let mockflows = Promise.resolve();
			if (mock) {
				mockflows = this.generateMockFlowsForEndpoint(
					group, parsed, bindSwagger)
					.then((tasks) => {
						// mush all the tasks together
						tasks.forEach(a => {
							alltasks = alltasks.concat(a.pre);
						});
						tasks.forEach(a => {
							alltasks = alltasks.concat(a.update);
						});
						tasks.forEach(a => {
							alltasks = alltasks.concat(a.post);
						});
					}).catch(reject);
			}
			mockflows.then(() => {
				// save the swagger in the expected format, cleaned and bound
				alltasks.push(async () => {
					let data;
					if (formatType === 'json') {
						data = JSON.stringify(bindSwagger, null, 2);
					} else {
						data = YAML.safeDump(bindSwagger);
					}

					return await writeUniqueFile(
						this.endpointsDir,
						group,
						formatType,
						data
					);
				});

				// Write the files
				async.series(alltasks, (err, results) => {
					if (err) {
						this.logger.error(err);
						if (err instanceof ExtendableError) {
							return reject(err);
						} else {
							return reject(new InternalServerError());
						}
					}
					// get ID from the last results to resolve (endpoint fn)
					const id = path.basename(results.pop())
						.replace(/\.(json|yaml|yml)/, '');
					return resolve(id);
				});
			}).catch(reject);
		});
	}

	/**
	 * Generate the endpoint.
	 * @param {object} params - params for endpoint
	 * @returns {Promise}
	 */
	async generateEndpoints(params) {
		if (!params || !params.name || !params.model) {
			throw new BadRequestError();
		}

		const { _internal } = this.ctx.apibuilder;
		if (!_internal.endpointsEnabled && !_internal.getApiSpecTypes().openapi) {
			// Endpoints are not enabled and oas-flow-trigger not installed
			throw new ForbiddenError();
		}

		let endpointId = safeFilename(params.name);
		const safeName = `${endpointId}.json`;

		const generator = this._createModelGenerator(params.model);
		const doc = generator.generateEndpoint(params.name, params.description);
		const flowDefinitions = generator.generateFlows(params.name);

		await mkdirIfNotExistsAsync(this.flowsDir);

		let destPath;
		if (_internal.endpointsEnabled) {
			destPath = path.join(this.endpointsDir, safeName);
			await mkdirIfNotExistsAsync(this.endpointsDir);
		} else {
			destPath = path.join(this.openapiDir, safeName);
			// REFACTOR: when we can use fs.promises, we can use `recursive`
			// and remove this mkdir for apidocsDir.
			await mkdirIfNotExistsAsync(this.apidocsDir);
			await mkdirIfNotExistsAsync(this.openapiDir);

			const triggerYAML = generator.generateTriggers();
			await mkdirIfNotExistsAsync(this.triggersDir);
			await afs.writeFile(
				path.join(this.triggersDir, `openapi-${endpointId}.yaml`),
				triggerYAML
			);
			// The openapi path has a different URL segment
			endpointId = `openapi-${endpointId}`;
		}
		await afs.writeFile(destPath, JSON.stringify(doc, null, '\t'));
		for (const id in flowDefinitions) {
			const file = path.join(this.flowsDir, `${id}.json`);
			await this.flowManager.saveFlow(id, file, flowDefinitions[id]);
		}
		return endpointId;
	}

	/**
	 * Produces mock flows for a swagger definition.
	 * @param {object} swagger - a dereferenced swagger document.
	 * @returns {Promise}
	 */
	mockEndpoints(swagger) {
		return new Promise((resolve, reject) => {
			if (!swagger || typeof swagger !== 'object') {
				return reject(new BadRequestError('Missing parameter'));
			}
			const data = [];
			Object.keys(swagger.paths).forEach((spath) => {
				if (spath.startsWith('x-')) {
					return;
				}
				Object.keys(swagger.paths[spath]).forEach((verb) => {
					if (verb.startsWith('x-') || verb === 'parameters') {
						return;
					}
					const operation = swagger.paths[spath][verb];
					const operationId = getOperationId(operation, spath, verb);
					this.logger.trace(`mocking endpoint ${verb} ${spath} as "${operationId}"`);

					const codes = Object.keys(operation.responses)
						.sort((a, b) => {
							if (b.startsWith('x-')) {
								return -1;
							}
							// there "should be one response code" and that should have
							// precedence over 'default'.
							if (a < b || b === 'default') {
								return -1;
							}
							return 0;
						});
					const code = codes.shift(); // pull first code
					const response = operation.responses[code];
					const mock = mockResponse(swagger, operationId, code, response);
					data.push({
						path: spath,
						method: verb,
						flow: mock
					});
				});
			});
			resolve(data);
		});
	}

	/**
	 * Generate mock flows for an endpoint.
	 * @param {string} group - The group name, i.e. safeFilename(swagger.info.title)
	 * @param {object} swagger - The resolved swagger to mock.
	 * @param {object} bindSwagger - The original swagger document to bind.
	 * @return {Promise<array>} A promise to the array of tasks (pre, update, post).
	 */
	generateMockFlowsForEndpoint(group, swagger, bindSwagger) {
		return new Promise((resolve, reject) => {
			// mock all the endpoints
			return this.mockEndpoints(swagger).then((data) => {
				// for each endpoint mocked, bind to the flow which generates all of the
				// tasks to complete the operation.  the 'pre' tasks will check for files;
				// the 'update' tasks will write files; and the 'post' tasks will update
				// the endpoint.
				return Promise.all(data.map((item) => {
					return this._bindFlowToEndpoint(bindSwagger, {
						group,
						...item,
						tasks: {
							pre: [],
							update: [],
							post: []
						}
					});
				})).then(resolve, reject);
			});
		});
	}

	/*
	 * Returns an array of summaries (group name, endpoint count)
	 * for each API group
	 */
	async getEndpointSummaries() {
		// TODO(AH): As part of the clean up:
		//   * change function name getAPIDocSummaries

		const { _internal } = this.ctx.apibuilder;
		const { spec, url } = _internal.getDynamicOpenAPI();
		// get legacy endpoints
		const items = [];
		const endpoints = this.ctx.apibuilder.getEndpoints();
		for (const apiName in endpoints) {
			// A list of keys to be excluded when counting the endpoints.
			// This resolves the issue when the parameters are attached to
			// root level of the endpoint - [RDPP-4818].
			const { endpoint } = endpoints[apiName];
			items.push({
				id: apiName,
				name: endpoint.info.title,
				type: LEGACY_SPEC_TYPE,
				description: endpoint.info.description,
				url: `${url}?endpoints/${apiName}`,
				status: !endpoint.hasOwnProperty('x-enabled') ? {
					enabled: true
				} : {
					...endpoint['x-enabled']
				}
			});
		}

		// get summary of loaded apiSpecs.
		const apiSpecs = _internal.getApiSpecs();
		const { triggers: allTriggers } = _internal.getTriggerManager().getInfo();

		for (const apiSpec of apiSpecs) {
			const { triggerTypeId } = apiSpec;
			const status = { enabled: true };

			// check trigger for validity
			for (const flowTriggerId in allTriggers[triggerTypeId]) {
				const trigger = allTriggers[triggerTypeId][flowTriggerId];
				if (!trigger.status.valid) {
					status.enabled = false;
					status.errors = status.errors || [];
					status.errors.push(`${trigger.flowId}:  ${trigger.status.error}`);
				}
			}

			items.push({
				id: triggerTypeId,
				name: apiSpec.name,
				type: apiSpec.type,
				description: apiSpec.description,
				url: apiSpec.url,
				status,
				// Only apiSpecs that have triggers will have triggerTypeId.
				// We can't assume id will always be the triggerTypeId, especially when we have
				// "dynamic" and endpoints so it's best to be explicit.
				triggerTypeId
			});
		}

		// Add the dynamically generated spec.
		items.push({
			id: DYNAMIC_SPEC_TYPE,
			name: spec.info.title,
			type: DYNAMIC_SPEC_TYPE,
			description: spec.info.description,
			url,
			status: { enabled: true }
		});

		return items;
	}

	/**
	 * Get API endpoints
	 * @param {string} groupName - The name of the API endpoint group
	 * @returns {Promise<Object>} An endpoint information object
	 */
	async getEndpoints(groupName) {
		// TODO(AH): As part of the clean up:
		//   * change function name getAPIDoc
		//   * change group name to something more clear
		if (!groupName) {
			throw new BadRequestError();
		}

		const { _internal } = this.ctx.apibuilder;

		// Sucks, but we have a "generated" type too which is different
		// from specs and endpoints. Maybe we can consider moving this to a
		// separate api and separate route in the UI if it's still bad
		// since this conflicts with any trigger called "generated", or any
		// endpoint called "generated"
		if (groupName === DYNAMIC_SPEC_TYPE) {
			// decide if we want to generate it once and reuse it. Would have to be
			// set by the validateSwagger task though, unless we generate it first
			// in a dedicated task
			const { spec, url } = _internal.getDynamicOpenAPI();
			return {
				endpoint: spec,
				name: spec.info.title,
				type: DYNAMIC_SPEC_TYPE,
				url
			};
		}

		if (_internal.endpointsEnabled) {
			const endpoints = this.ctx.apibuilder.getEndpoints();
			if (!endpoints.hasOwnProperty(groupName)) {
				throw new NotFoundError();
			}
			const { endpoint, format } = endpoints[groupName];
			const { url } = _internal.getDynamicOpenAPI();
			return {
				endpoint,
				name: endpoint.info.title,
				type: LEGACY_SPEC_TYPE,
				// Unused, can be deleted
				format,
				url: `${url}?endpoints/${groupName}`
			};
		} else {
			const apiSpecs = _internal.getApiSpecs();
			const specTypes = _internal.getApiSpecTypes();
			// check to see if there's a loaded spec for the type being requested
			const apiSpec = apiSpecs.find(s => s.triggerTypeId === groupName);
			if (!apiSpec) {
				throw new NotFoundError();
			}
			const specTypeName = apiSpec.type;
			const specType = specTypes[specTypeName];

			const { triggerTypeId } = apiSpec;
			// build a map of flows so that are triggered from this spec.
			const { triggers: allTriggers } = _internal.getTriggerManager().getInfo();
			const triggeredFlows = {};
			const invalidTriggers = {};
			for (const flowTriggerId in allTriggers[triggerTypeId]) {
				const trigger = allTriggers[triggerTypeId][flowTriggerId];
				const operationId = specType.getOperationId(trigger.triggerParameters);
				triggeredFlows[operationId] = trigger.flowId;
				if (!trigger.status.valid) {
					invalidTriggers[operationId] = {
						flowId: trigger.flowId,
						error: trigger.status.error
					};
				}
			}

			return {
				endpoint: apiSpec.definition,
				type: apiSpec.type,
				name: apiSpec.name,
				url: apiSpec.url,
				triggerTypeId,
				triggeredFlows,
				invalidTriggers
			};
		}
	}

	/**
	 * Updates an endpoint file.  WARNING: you should only ever call this using
	 * the original `swagger` source, not the in-memory endpoint because we
	 * want the original format, sans x-enabled.
	 *
	 * @async
	 * @private
	 * @param {object} endpointInfo - The endpoint information obtained from
	 * 		`APIBuilder._internal.getEndpointInfo`.
	 * @param {object} swagger - The Swagger to update.
	 * @return {Promise<undefined>}
	 */
	_updateEndpoint(endpointInfo, swagger) {
		return new Promise((resolve, reject) => {
			try {
				let data;
				if (endpointInfo.format === 'json') {
					data = JSON.stringify(swagger, null, 2);
				} else {
					data = YAML.safeDump(swagger);
				}
				writeFile(endpointInfo.filename, data, (err) => {
					if (err) {
						return reject(err);
					}
					resolve();
				});
			} catch (ex) {
				return reject(ex);
			}
		});
	}

	/**
	 * Delete a method from and api group on some endpoint.
	 * @async
	 * @param  {object} params - contains api group, path within the group, and method
	 * @returns {Promise}
	 */
	async deleteEndpoint(params = {}) {
		const missing = [ 'group', 'path', 'method' ].find(a => !params[a]);
		if (missing) {
			const paramError = new BadRequestError(`Missing parameter: ${missing}`);
			this.logger.error(paramError.message);
			throw paramError;
		}

		const { group, path, method } = params;

		const epInfo = this.ctx.apibuilder._internal.getEndpointInfo(group);
		if (!epInfo) {
			// not my first choice of error, but RDPP-1100 stipulates that if
			// an endpoint is not found, the response is InternalServerError.
			this.logger.error(`endpoint not found: ${group}`);
			throw new InternalServerError();
		}

		ensureEndpointMethodExists(epInfo.swagger, path, method); // throws
		delete epInfo.swagger.paths[path][method];

		return this._updateEndpoint(epInfo, epInfo.swagger)
			.catch((err) => {
				this.logger.error(err);
				throw new InternalServerError();
			});
	}

	/**
	 * Binds a flow to an endpoint (swagger) definition, and resolves with the set of
	 * tasks to complete that binding where pre-tasks check directories and existing
	 * files, update-tasks write the files, and post-tasks complete the binding.
	 *
	 * @param {object} endpoint - the endpoint to bind flow to
	 * @param {object} [params] - Bind parameters
	 * @param {string} params.group - The endpoint group name
	 * @param {string} params.path - The endpoint path to bind
	 * @param {string} params.method - The endpoint method to bind (e.g. "post")
	 * @param {object} params.tasks - An object of 3 arrays, `pre`, `update`, and `post`
	 * @param {object} params.flow - The flow object to bind.
	 * @return {Promise<object>} A promise to the tasks.
	 */
	_bindFlowToEndpoint(endpoint, params = {}) {
		return new Promise((resolve, reject) => {
			const missing = [ 'group', 'path', 'method', 'flow', 'tasks' ].find(a => !params[a]);
			if (missing) {
				const paramError = new BadRequestError(`Missing parameter: ${missing}`);
				this.logger.error(paramError.message);
				return reject(paramError);
			}

			const operation = endpoint.paths[params.path][params.method];
			const operationId = getOperationId(operation, params.path, params.method);
			const flowId = `${params.group}-${operationId}`;

			// Check that a flow with that ID does not exist.
			params.tasks.pre.push((next) => {
				this.controllers.flowsController.exists(flowId).then(exists => {
					if (exists) {
						this.logger.error(`Flow already exists: ${flowId}`);
						return next(new ForbiddenError('A flow already exists.'));
					} else if (operation['x-flow']) {
						this.logger.error(`Endpoint already bound to a flow: ${operation['x-flow']}`);
						return next(new ForbiddenError('Endpoint already bound to a flow.'));
					}
					if (!isSafeFilename(flowId)) {
						this.logger.error(`Not a safe flow filename: ${flowId}`);
						return next(new ForbiddenError(
							'Flow filename can only contain alpha-numeric, underscores, and dashes.'
						));
					}
					next();
				}).catch(next);
			});

			// Write the new flow
			params.tasks.update.push((next) => {
				// Update the endpoint flowId
				operation['x-flow'] = flowId;
				// save the flow
				this.controllers.flowsController.saveFlow(flowId, params.flow)
					.then(next)
					.catch(next);
			});

			resolve(params.tasks);
		});
	}

	/**
	 * Binds a flow to an existing endpoint file (swagger) definition.
	 *
	 * @param {object} [params] - Bind parameters
	 * @param {string} params.group - The endpoint group name
	 * @param {string} params.path - The endpoint path to bind
	 * @param {string} params.method - The endpoint method to bind (e.g. "post")
	 * @param {object} params.flow - The flow object to bind.
	 * @return {Promise<object>} A promise to the tasks.
	 */
	bindFlow(params = {}) {
		return new Promise((resolve, reject) => {
			const names = [ 'group', 'path', 'method', 'flow' ];
			const name = names.find(a => !params[a]);
			if (name) {
				const paramError = new BadRequestError(`Missing parameter: ${name}`);
				this.logger.error(paramError.message);
				return reject(paramError);
			}
			const { group, path, method } = params;

			const epInfo = this.ctx.apibuilder._internal.getEndpointInfo(group);
			if (!epInfo) {
				// not my first choice of error, but RDPP-1100 stipulates that if
				// an endpoint is not found, the response is InternalServerError.
				return reject(new InternalServerError());
			}

			// Ensure the method to bind exists
			ensureEndpointMethodExists(epInfo.swagger, path, method); // throws

			this._bindFlowToEndpoint(epInfo.swagger, {
				...params,
				tasks: {
					pre: [
						// pre-task to check directory
						(next) => mkdirIfNotExists(this.flowsDir, next)
					],
					update: [],
					post: [
						// final write to endpoint (this is done knowing that
						// _bindFlowToEndpoint does not write any post events).
						(next) => {
							this._updateEndpoint(epInfo, epInfo.swagger).then(next);
						}
					]
				}
			}).then((tasks) => {
				const alltasks = tasks.pre.concat(tasks.update).concat(tasks.post);
				return async.series(alltasks, (err) => {
					if (err) {
						this.logger.error(err);
						if (err instanceof ExtendableError) {
							reject(err);
						} else {
							reject(new InternalServerError());
						}
					} else {
						const flowId
							= epInfo.swagger.paths[params.path][params.method]['x-flow'];
						resolve(flowId);
					}
				});
			});
		});
	}
}

module.exports = EndpointsController;
